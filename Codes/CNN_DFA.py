# -*- coding: utf-8 -*-
"""CNN_ssdfa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yX7hXHyC7nlXDHM8Wg6KfuO49jQxdwha
"""

import argparse
import os
import sys

parser = argparse.ArgumentParser()

parser.add_argument("--epochs", default=50, type=int)
parser.add_argument("--batch_size", type=int, default=64)
parser.add_argument("--lr", type=float, default=1e-4)
parser.add_argument("--eps", type=float, default=1e-5)
parser.add_argument("--dropout", type=float, default=0.5)
parser.add_argument("--act", type=str, default="relu")
parser.add_argument("--bias", type=float, default=0.0)
parser.add_argument("--gpu", type=int, default=0)
parser.add_argument("--dfa", type=int, default=0)
parser.add_argument("--sparse", type=int, default=0)
parser.add_argument("--rank", type=int, default=0)
parser.add_argument("--init", type=str, default="glorot_uniform")
parser.add_argument("--save", type=int, default=0)
parser.add_argument("--name", type=str, default="cifar10_fc")
parser.add_argument("--load", type=str, default=None)

args = parser.parse_args(args=[])

print(args.lr)
print(args.batch_size)


import math

import keras
import numpy as np

# import tensorflow as tf
import tensorflow.compat.v1 as tf

"""# Libraries

## Activation
"""

# import numpy as np
# import tensorflow as tf


class Activation(object):
    def forward(self, x):
        pass

    def gradient(self, x):
        pass


class Sigmoid(Activation):
    def __init__(self):
        pass

    def forward(self, x):
        return tf.sigmoid(x)

    def sigmoid_gradient(self, x):
        sig = tf.sigmoid(x)
        return tf.multiply(sig, tf.subtract(1.0, sig))

    def gradient(self, x):
        return tf.multiply(x, tf.subtract(1.0, x))


class Relu(Activation):
    def __init__(self):
        pass

    def forward(self, x):
        return tf.nn.relu(x)

    def gradient(self, x):
        # pretty sure this gradient works for A and Z
        return tf.cast(x > 0.0, dtype=tf.float32)


# https://theclevermachine.wordpress.com/tag/tanh-function/
class Tanh(Activation):
    def __init__(self):
        pass

    def forward(self, x):
        return tf.tanh(x)

    def gradient(self, x):
        # this is gradient wtf A, not Z
        return 1 - tf.pow(x, 2)


# https://medium.com/@aerinykim/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d
# /home/brian/tensorflow/tensorflow/python/ops/nn_grad ... grep "_SoftmaxGrad"


class Softmax(Activation):
    def __init__(self):
        pass

    def forward(self, x):
        return tf.softmax(x)

    # this is gradient for A
    def gradient(self, x):
        # this is impossible and not bio plausible
        assert False

        flat = tf.reshape(x, [-1])
        diagflat = tf.diag(flat)
        dot = tf.matmul(flat, tf.transpose(flat))
        return diagflag - dot


class LeakyRelu(Activation):
    def __init__(self, leak=0.2):
        self.leak = leak

    def forward(self, x):
        return tf.nn.leaky_relu(x, alpha=self.leak)

    def gradient(self, x):
        # pretty sure this gradient works for A and Z
        return tf.add(
            tf.cast(x > 0.0, dtype=tf.float32),
            tf.cast(x < 0.0, dtype=tf.float32) * self.leak,
        )


class SqrtRelu(Activation):
    def __init__(self):
        pass

    def forward(self, x):
        return tf.sqrt(tf.nn.relu(x))

    def gradient(self, x):
        # pretty sure this gradient works for A and Z
        return tf.cast(x > 0.0, dtype=tf.float32)


class Linear(Activation):
    def __init__(self):
        pass

    def forward(self, x):
        return x

    def gradient(self, x):
        return tf.ones(shape=tf.shape(x))


"""## Layer"""

# import tensorflow as tf
# import numpy as np


class Layer:
    def __init__(self):
        super().__init__()

    ###################################################################

    def get_weights(self):
        assert False

    def num_params(self):
        assert False

    def forward(self, X):
        assert False

    ###################################################################

    def bp(self, AI, AO, DO, cache):
        assert False

    def dfa(self, AI, AO, E, DO, cache):
        assert False

    def lel(self, AI, AO, DO, Y, cache):
        assert False

    ###################################################################


"""## conv_utils"""

# import numpy as np


def conv_output_length(input_length, filter_size, padding, stride, dilation=1):
    """Determines output length of a convolution given input length.

    Arguments:
        input_length: integer.
        filter_size: integer.
        padding: one of "same", "valid", "full".
        stride: integer.
        dilation: dilation rate, integer.

    Returns:
        The output length (integer).
    """
    if input_length is None:
        return None
    assert padding in {"same", "valid", "full"}
    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)
    if padding == "same":
        output_length = input_length
    elif padding == "valid":
        output_length = input_length - dilated_filter_size + 1
    elif padding == "full":
        output_length = input_length + dilated_filter_size - 1
    return (output_length + stride - 1) // stride


def conv_input_length(output_length, filter_size, padding, stride):
    """Determines input length of a convolution given output length.

    Arguments:
        output_length: integer.
        filter_size: integer.
        padding: one of "same", "valid", "full".
        stride: integer.

    Returns:
        The input length (integer).
    """
    if output_length is None:
        return None
    assert padding in {"same", "valid", "full"}
    if padding == "same":
        pad = filter_size // 2
    elif padding == "valid":
        pad = 0
    elif padding == "full":
        pad = filter_size - 1
    return (output_length - 1) * stride - 2 * pad + filter_size


"""## AvgPool"""

import math

from tensorflow.python.ops import gen_nn_ops

# from conv_utils import conv_output_length
# from conv_utils import conv_input_length

# /home/brian/environments/py3/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py
# def avg_pool_grad(orig_input_shape, grad, ksize, strides, padding, data_format="NHWC", name=None):


class AvgPool(Layer):
    def __init__(self, size, ksize, strides, padding):
        self.size = size
        self.batch_size, self.h, self.w, self.fin = self.size

        self.ksize = ksize
        _, self.kh, self.kw, _ = self.ksize

        self.strides = strides
        _, self.sh, self.sw, _ = self.strides

        self.padding = padding

    ###################################################################

    def get_weights(self):
        return []

    def output_shape(self):
        oh = conv_output_length(self.h, self.kh, self.padding.lower(), self.sh)
        ow = conv_output_length(self.w, self.kw, self.padding.lower(), self.sw)
        od = self.fin
        return [oh, oh, od]

    def num_params(self):
        return 0

    def forward(self, X):
        A = tf.nn.avg_pool(
            X, ksize=self.ksize, strides=self.strides, padding=self.padding
        )
        return {"aout": A, "cache": {}}

        ###################################################################

    def backward(self, AI, AO, DO, cache):
        DI = gen_nn_ops.avg_pool_grad(
            orig_input_shape=self.size,
            grad=DO,
            ksize=self.ksize,
            strides=self.strides,
            padding=self.padding,
        )
        return {"dout": DI, "cache": {}}

    def gv(self, AI, AO, DO, cache):
        return []

    def train(self, AI, AO, DO):
        return []

    ###################################################################

    def dfa_backward(self, AI, AO, E, DO):
        grad = gen_nn_ops.avg_pool_grad(
            orig_input_shape=self.size,
            grad=DO,
            ksize=self.ksize,
            strides=self.strides,
            padding=self.padding,
        )
        return grad

    def dfa_gv(self, AI, AO, E, DO):
        return []

    def dfa(self, AI, AO, E, DO):
        return []

    ###################################################################

    def lel_backward(self, AI, AO, E, DO, cache):
        return self.backward(AI, AO, DO, cache)

    def lel_gv(self, AI, AO, E, DO, Y, cache):
        return []

    def lel(self, AI, AO, E, DO, Y):
        return []

    ###################################################################


"""## MaxPool"""

from tensorflow.python.ops import gen_nn_ops


class MaxPool(Layer):
    def __init__(self, size, ksize, strides, padding):
        self.size = size
        self.batch_size, self.h, self.w, self.fin = self.size

        self.ksize = ksize
        _, self.kh, self.kw, _ = self.ksize

        self.strides = strides
        _, self.sh, self.sw, _ = self.strides

        self.padding = padding

    ###################################################################

    def get_weights(self):
        return []

    def output_shape(self):
        oh = conv_output_length(self.h, self.kh, self.padding.lower(), self.sh)
        ow = conv_output_length(self.w, self.kw, self.padding.lower(), self.sw)
        od = self.fin
        return [oh, oh, od]

    def num_params(self):
        return 0

    def forward(self, X):
        A = tf.nn.max_pool(
            X, ksize=self.ksize, strides=self.strides, padding=self.padding
        )
        return {"aout": A, "cache": {}}

    ###################################################################

    def bp(self, AI, AO, DO, cache):
        DI = gen_nn_ops.max_pool_grad(
            grad=DO,
            orig_input=AI,
            orig_output=AO,
            ksize=self.ksize,
            strides=self.strides,
            padding=self.padding,
        )
        return {"dout": DI, "cache": {}}, []

    def dfa(self, AI, AO, E, DO, cache):
        return self.bp(AI, AO, DO, cache)

    def lel(self, AI, AO, DO, Y, cache):
        return self.bp(AI, AO, DO, cache)

    ###################################################################


"""## BatchNorm"""

from tensorflow.python.ops import gen_nn_ops


class BatchNorm(Layer):
    def __init__(self, input_size, name=None, load=None, train=True, eps=1e-3):
        self.input_size = list(input_size)
        if len(self.input_size) == 2:
            self.dims = [0]
        elif len(self.input_size) == 4:
            self.dims = [0, 1, 2]
        else:
            assert False
        self.size = self.input_size[-1]

        self.name = name
        self._train = train
        self.eps = eps
        self.num_parameters = np.prod(self.size) * 2

        if load:
            print("Loading Weights: " + self.name)
            weight_dict = np.load(load).item()
            gamma = weight_dict[self.name + "_gamma"]
            beta = weight_dict[self.name + "_beta"]

            if np.shape(gamma) != (self.size,):
                print(np.shape(gamma), self.size)
                assert np.shape(gamma) == (self.size,)

            if np.shape(beta) != (self.size,):
                print(np.shape(beta), self.size)
                assert np.shape(beta) == (self.size,)

        else:
            gamma = np.ones(shape=self.size)
            beta = np.zeros(shape=self.size)

        self.gamma = tf.Variable(gamma, dtype=tf.float32)
        self.beta = tf.Variable(beta, dtype=tf.float32)

    ###################################################################

    def get_weights(self):
        return [(self.name + "_gamma", self.gamma), (self.name + "_beta", self.beta)]

    def num_params(self):
        return self.num_parameters

    def output_shape(self):
        if len(self.input_size) == 2:
            return self.input_size[1]
        elif len(self.input_size) == 4:
            return self.input_size[1:4]
        else:
            assert False

    ###################################################################

    def forward(self, X):
        mean = tf.reduce_mean(X, axis=self.dims)
        _, var = tf.nn.moments(X - mean, axes=self.dims)
        A = tf.nn.batch_normalization(
            x=X,
            mean=mean,
            variance=var,
            offset=self.beta,
            scale=self.gamma,
            variance_epsilon=self.eps,
        )
        return {"aout": A, "cache": {}}

    def backward(self, AI, AO, DO, cache=None):
        mean = tf.reduce_mean(AI, axis=self.dims)
        _, var = tf.nn.moments(AI - mean, axes=self.dims)
        ivar = 1.0 / tf.sqrt(self.eps + var)

        if len(self.input_size) == 2:
            AI = tf.reshape(AI, (self.input_size[0], 1, 1, self.input_size[1]))
            DO = tf.reshape(AI, (self.input_size[0], 1, 1, self.size))

        [DI, dgamma, dbeta, _, _] = gen_nn_ops.fused_batch_norm_grad_v2(
            y_backprop=DO,
            x=AI,
            scale=self.gamma,
            reserve_space_1=mean,
            reserve_space_2=ivar,
            epsilon=self.eps,
            is_training=True,
        )

        if len(self.input_size) == 2:
            DI = tf.reshape(DI, (self.input_size[0], self.size))

        return {"dout": DI, "cache": {}}

    def gv(self, AI, AO, DO, cache=None):
        if not self._train:
            return []

        mean = tf.reduce_mean(AI, axis=self.dims)
        _, var = tf.nn.moments(AI - mean, axes=self.dims)
        ivar = 1.0 / tf.sqrt(self.eps + var)

        if len(self.input_size) == 2:
            AI = tf.reshape(AI, (self.input_size[0], 1, 1, self.input_size[1]))
            DO = tf.reshape(AI, (self.input_size[0], 1, 1, self.size))

        [DI, dgamma, dbeta, _, _] = gen_nn_ops.fused_batch_norm_grad_v2(
            y_backprop=DO,
            x=AI,
            scale=self.gamma,
            reserve_space_1=mean,
            reserve_space_2=ivar,
            epsilon=self.eps,
            is_training=True,
        )

        if len(self.input_size) == 2:
            DI = tf.reshape(DI, (self.input_size[0], self.size))

        return [(dgamma, self.gamma), (dbeta, self.beta)]

    ###################################################################

    def dfa_backward(self, AI, AO, E, DO):
        return self.backward(AI, AO, DO)

    def dfa_gv(self, AI, AO, E, DO):
        return self.gv(AI, AO, DO)

    ###################################################################

    def lel_backward(self, AI, AO, E, DO, Y, cache):
        return self.backward(AI, AO, DO, cache)

    def lel_gv(self, AI, AO, E, DO, Y, cache):
        return self.gv(AI, AO, DO, cache)

    ###################################################################


"""## ConvToFullyConnected"""


class ConvToFullyConnected(Layer):
    def __init__(self, input_shape):
        self.shape = input_shape

    ###################################################################

    def get_weights(self):
        return []

    def output_shape(self):
        return np.prod(self.shape)

    def num_params(self):
        return 0

    def forward(self, X):
        A = tf.reshape(X, [tf.shape(X)[0], -1])
        return {"aout": A, "cache": {}}

    ###################################################################

    def bp(self, AI, AO, DO, cache):
        DI = tf.reshape(DO, [tf.shape(AI)[0]] + self.shape)
        return {"dout": DI, "cache": {}}, []

    def dfa(self, AI, AO, E, DO, cache):
        return self.bp(AI, AO, DO, cache)

    def lel(self, AI, AO, DO, Y, cache):
        return self.bp(AI, AO, DO, cache)

    ###################################################################


"""## init_tensor"""


def init_matrix(size, init, std=None):
    input_size, output_size = size

    if init == "zero":
        weights = np.zeros(shape=(input_size, output_size))

    elif init == "sqrt_fan_in":
        sqrt_fan_in = np.sqrt(input_size)
        weights = np.random.uniform(
            low=-1.0 / sqrt_fan_in,
            high=1.0 / sqrt_fan_in,
            size=(input_size, output_size),
        )

    elif init == "glorot_uniform":
        limit = np.sqrt(6.0 / (input_size + output_size))
        weights = np.random.uniform(
            low=-limit, high=limit, size=(input_size, output_size)
        )

    elif init == "glorot_normal":
        scale = np.sqrt(2.0 / (input_size + output_size))
        weights = np.random.normal(loc=0.0, scale=scale, size=(input_size, output_size))

    elif init == "alexnet":
        weights = np.random.normal(loc=0.0, scale=0.01, size=(input_size, output_size))

    elif init == "normal":
        scale = std
        weights = np.random.normal(loc=0.0, scale=scale, size=(input_size, output_size))

    else:
        weights = np.random.normal(loc=0.0, scale=1.0, size=(input_size, output_size))

    return weights


#######################################


def init_filters(size, init, std=None):
    fh, fw, fin, fout = size

    if init == "zero":
        weights = np.zeros(shape=(fh, fw, fin, fout))

    elif init == "sqrt_fan_in":
        assert False

    elif init == "glorot_uniform":
        limit = np.sqrt(6.0 / (fh * fw * fin + fh * fw * fout))
        weights = np.random.uniform(low=-limit, high=limit, size=(fh, fw, fin, fout))

    elif init == "glorot_normal":
        scale = np.sqrt(2.0 / (fh * fw * fin + fh * fw * fout))
        weights = np.random.normal(loc=0.0, scale=scale, size=(fh, fw, fin, fout))

    elif init == "alexnet":
        weights = np.random.normal(loc=0.0, scale=0.01, size=(fh, fw, fin, fout))

    elif init == "normal":
        scale = std
        weights = np.random.normal(loc=0.0, scale=scale, size=(fh, fw, fin, fout))

    else:
        assert False

    return weights


#######################################

"""## Convolution"""


class Convolution(Layer):
    def __init__(
        self,
        input_shape,
        filter_sizes,
        init,
        strides=[1, 1, 1, 1],
        padding="SAME",
        activation=None,
        bias=0.0,
        use_bias=True,
        name=None,
        load=None,
        train=True,
    ):
        self.input_shape = input_shape
        self.filter_sizes = filter_sizes
        self.batch_size, self.h, self.w, self.fin = self.input_shape
        self.fh, self.fw, self.fin, self.fout = self.filter_sizes
        self.init = init
        self.strides = strides
        _, self.sh, self.sw, _ = self.strides
        self.padding = padding
        self.activation = Linear() if activation == None else activation
        self.use_bias = use_bias
        self.name = name
        self.train_flag = train

        if load:
            print("Loading Weights: " + self.name)
            weight_dict = np.load(load, encoding="latin1", allow_pickle=True).item()
            filters = weight_dict[self.name]
            bias = weight_dict[self.name + "_bias"]
        else:
            filters = init_filters(size=self.filter_sizes, init=self.init)
            bias = np.ones(shape=self.fout) * bias

        self.filters = tf.Variable(filters, dtype=tf.float32)
        self.bias = tf.Variable(bias, dtype=tf.float32)

    ###################################################################

    def get_weights(self):
        return [(self.name, self.filters), (self.name + "_bias", self.bias)]

    def output_shape(self):
        oh = conv_output_length(self.h, self.fh, self.padding.lower(), self.sh)
        ow = conv_output_length(self.w, self.fw, self.padding.lower(), self.sw)
        od = self.fout
        return [oh, oh, od]

    def num_params(self):
        filter_weights_size = self.fh * self.fw * self.fin * self.fout
        bias_weights_size = self.fout
        return filter_weights_size + bias_weights_size

    def forward(self, X):
        Z = tf.nn.conv2d(X, self.filters, self.strides, self.padding)
        if self.use_bias:
            Z = Z + tf.reshape(self.bias, (1, 1, 1, self.fout))

        A = self.activation.forward(Z)
        return {"aout": A, "cache": {}}

    ###################################################################

    def bp(self, AI, AO, DO, cache):
        DO = tf.multiply(DO, self.activation.gradient(AO))
        DI = tf.nn.conv2d_backprop_input(
            input_sizes=self.input_shape,
            filter=self.filters,
            out_backprop=DO,
            strides=self.strides,
            padding=self.padding,
        )

        DF = tf.nn.conv2d_backprop_filter(
            input=AI,
            filter_sizes=self.filter_sizes,
            out_backprop=DO,
            strides=self.strides,
            padding=self.padding,
        )
        DB = tf.reduce_sum(DO, axis=[0, 1, 2])

        if self.train_flag:
            return {"dout": DI, "cache": {}}, [(DF, self.filters), (DB, self.bias)]
        else:
            return {"dout": DI, "cache": {}}, []

    def dfa(self, AI, AO, E, DO, cache):
        return self.bp(AI, AO, DO, cache)

    def lel(self, AI, AO, DO, Y, cache):
        return self.bp(AI, AO, DO, cache)

    ###################################################################


"""## Dropout"""


class Dropout(Layer):
    def __init__(self, rate):
        self.rate = rate

    ###################################################################

    def get_weights(self):
        return []

    def num_params(self):
        return 0

    def forward(self, X):
        self.dropout_mask = tf.cast(
            tf.random_uniform(shape=tf.shape(X)) > self.rate, tf.float32
        )
        # self.dropout_mask = tf.cast(np.random.binomial(size=X.shape, n=1, p=1 - self.rate))
        A = X * self.dropout_mask
        return {"aout": A, "cache": {}}

    ###################################################################

    def bp(self, AI, AO, DO, cache):
        DI = DO * self.dropout_mask
        return {"dout": DI, "cache": {}}, []

    def dfa(self, AI, AO, E, DO, cache):
        return self.bp(AI, AO, DO, cache)

    def lel(self, AI, AO, DO, Y, cache):
        return self.bp(AI, AO, DO, cache)

    ###################################################################


"""## FeedbackMatrix"""


def matrix_rank(mat):
    return np.linalg.matrix_rank(mat)


def matrix_sparsity(mat):
    # we want to make sure all the [10]s have some sparisty.
    # so we transpose the matrix from [10, 100] -> [100, 10] so that way we can look at the first [10]
    # summing along the 0 axis, sums along the 10s ... so we get a [100] vector sum

    a = np.sum(mat.T[0] != 0)
    assert np.all(np.sum(mat != 0, axis=0) == a)
    return a


def full_feedback(mat):
    # sum along the opposite axis here.

    return np.all(np.sum(mat != 0, axis=1) > 0)


def FeedbackMatrix(size: tuple, sparse: int, rank: int):
    input_size, output_size = size

    if sparse:
        sqrt_fan_out = np.sqrt(1.0 * output_size / np.sqrt(input_size * sparse))
        # sqrt_fan_out = np.sqrt(1.0 * output_size / input_size * sparse)
        # sqrt_fan_out = np.sqrt(output_size)
    else:
        sqrt_fan_out = np.sqrt(output_size)

    high = 1.0 / sqrt_fan_out
    low = -high

    fb = np.zeros(shape=size)
    fb = np.transpose(fb)

    choices = range(input_size)
    counts = np.zeros(input_size)
    total_connects = 1.0 * sparse * rank
    connects_per = 1.0 * sparse * rank / input_size

    idxs = []

    if sparse and rank:
        assert sparse * rank >= input_size

        # pick rank sets of sparse indexes
        for ii in range(rank):
            remaining_connects = total_connects - np.sum(counts)
            pdf = (connects_per - counts) / remaining_connects
            pdf = np.clip(pdf, 1e-6, 1.0)
            pdf = pdf / np.sum(pdf)

            choice = np.random.choice(choices, sparse, replace=False, p=pdf)
            counts[choice] += 1.0
            idxs.append(choice)

            # create our masks
        masks = []
        for ii in range(rank):
            masks.append(np.zeros(shape=(output_size, input_size)))

        for ii in range(output_size):
            choice = np.random.choice(range(len(idxs)))
            idx = idxs[choice]
            masks[choice][ii][idx] = 1.0

        # multiply mask by random rank 1 matrix.
        for ii in range(rank):
            tmp1 = np.random.uniform(low, high, size=(output_size, 1))
            tmp2 = np.random.uniform(low, high, size=(1, input_size))
            fb = fb + masks[ii] * np.dot(tmp1, tmp2)

        # rank fix
        fb = fb * (high / np.std(fb))
        fb = fb.T

    elif sparse:
        mask = np.zeros(shape=(output_size, input_size))
        for ii in range(output_size):
            idx = np.random.choice(choices, size=sparse, replace=False)
            mask[ii][idx] = 1.0

        mask = mask.T
        fb = np.random.uniform(low, high, size=(input_size, output_size))
        fb = fb * mask
        """
    mask = mask.T
    fb = np.random.uniform(0.5 / sqrt_fan_out, 2. / sqrt_fan_out, size=(input_size, output_size))
    fb = fb * mask

    sign = np.random.choice([-1., 1.], size=np.prod(np.shape(fb)), replace=True)
    sign = np.reshape(sign, newshape=np.shape(fb))
    fb = fb * sign
    """
    elif rank:
        fb = np.zeros(shape=(input_size, output_size))
        for ii in range(rank):
            tmp1 = np.random.uniform(low, high, size=(input_size, 1))
            tmp2 = np.random.uniform(low, high, size=(1, output_size))
            fb = fb + np.dot(tmp1, tmp2)
        # rank fix
        fb = fb * (high / np.std(fb))
    else:
        fb = np.random.uniform(low, high, size=(input_size, output_size))

    return fb


"""
size = (10, 100)
rank = 4
sparse = 3

B = FeedbackMatrix(size, sparse, rank)

for rank in range(10):
    for sparse in range(10):
        if rank * sparse > 10:
            B = FeedbackMatrix(size, sparse, rank)
            passed = (sparse == matrix_sparsity(B)) and (rank == matrix_rank(B)) and full_feedback(B)
            print (rank, sparse, passed)

            # if not passed:
            #    print (np.sum(B != 0, axis=0))

for rank in range(10):
    B = FeedbackMatrix(size, 0, rank)
    passed = (10 == matrix_sparsity(B)) and (rank == matrix_rank(B)) and full_feedback(B)
    print (rank, 0, passed)

for sparse in range(10):
    B = FeedbackMatrix(size, sparse, 0)
    passed = (sparse == matrix_sparsity(B)) and (10 == matrix_rank(B)) and full_feedback(B)
    print (0, sparse, passed)
"""

"""## FeedbackConv"""


class FeedbackConv(Layer):
    def __init__(self, size, num_classes, sparse, rank, name=None):
        self.size = size
        self.batch_size, self.h, self.w, self.f = self.size
        self.num_output = self.h * self.w * self.f
        self.num_classes = num_classes
        self.sparse = sparse
        self.rank = rank
        self.name = name

        b = FeedbackMatrix(
            size=(self.num_classes, self.num_output), sparse=self.sparse, rank=self.rank
        )
        self.B = tf.cast(tf.Variable(b), tf.float32)

        ###################################################################

    def get_weights(self):
        return [(self.name, self.B)]

    def num_params(self):
        return 0

    def forward(self, X):
        A = X
        return {"aout": A, "cache": {}}

        ###################################################################

    def bp(self, AI, AO, DO, cache):
        DI = DO
        return {"dout": DI, "cache": {}}, []

    def dfa(self, AI, AO, E, DO, cache):
        DI = tf.matmul(E, self.B)
        DI = tf.reshape(DI, self.size)
        return {"dout": DI, "cache": {}}, []

    def lel(self, AI, AO, DO, Y, cache):
        shape = tf.shape(AO)
        N = shape[0]
        AO = tf.reshape(AO, (N, self.num_output))
        S = tf.matmul(AO, tf.transpose(self.B))
        ES = tf.subtract(tf.nn.softmax(S), Y)
        DI = tf.matmul(ES, self.B)
        DI = tf.reshape(DI, self.size)
        return {"dout": DI, "cache": {}}, []


###################################################################

"""## FeedbackFC"""

np.set_printoptions(threshold=np.inf)


class FeedbackFC(Layer):
    num = 0

    def __init__(self, size, num_classes, sparse, rank, name=None):
        self.size = size
        self.input_size, self.output_size = self.size
        self.num_classes = num_classes
        self.sparse = sparse
        self.rank = rank
        self.name = name

        b = FeedbackMatrix(
            size=(self.num_classes, self.output_size),
            sparse=self.sparse,
            rank=self.rank,
        )
        self.B = tf.cast(tf.Variable(b), tf.float32)

    def get_weights(self):
        return [(self.name, self.B)]

    def num_params(self):
        return 0

    def forward(self, X):
        A = X
        return {"aout": A, "cache": {}}

    ###################################################################

    def bp(self, AI, AO, DO, cache):
        DI = DO
        return {"dout": DI, "cache": {}}, []

    def dfa(self, AI, AO, E, DO, cache):
        DI = tf.matmul(E, self.B)
        return {"dout": DI, "cache": {}}, []

    def lel(self, AI, AO, DO, Y, cache):
        S = tf.matmul(AO, tf.transpose(self.B))
        ES = tf.subtract(tf.nn.softmax(S), Y)
        DI = tf.matmul(ES, self.B)
        return {"dout": DI, "cache": {}}, []

        ###################################################################


"""## FullyConnected"""


class FullyConnected(Layer):
    def __init__(
        self,
        input_shape,
        size,
        init,
        activation=None,
        bias=0.0,
        use_bias=True,
        name=None,
        load=None,
        train=True,
    ):
        self.input_size = input_shape
        self.output_size = size
        self.init = init
        self.activation = Linear() if activation == None else activation
        self.name = name
        self.train_flag = train
        self.use_bias = use_bias

        if load:
            print("Loading Weights: " + self.name)
            weight_dict = np.load(load, encoding="latin1", allow_pickle=True).item()
            weights = weight_dict[self.name]
            bias = weight_dict[self.name + "_bias"]
        else:
            bias = np.ones(shape=self.output_size) * bias
            weights = init_matrix(
                size=(self.input_size, self.output_size), init=self.init
            )

        self.weights = tf.Variable(weights, dtype=tf.float32)
        self.bias = tf.Variable(bias, dtype=tf.float32)

    ###################################################################

    def get_weights(self):
        return [(self.name, self.weights), (self.name + "_bias", self.bias)]

    def num_params(self):
        weights_size = self.input_size * self.output_size
        bias_size = self.output_size
        return weights_size + bias_size

    def forward(self, X):
        Z = tf.matmul(X, self.weights)
        if self.use_bias:
            Z = Z + self.bias
        A = self.activation.forward(Z)
        return {"aout": A, "cache": {}}

    ###################################################################

    def bp(self, AI, AO, DO, cache):
        DO = tf.multiply(DO, self.activation.gradient(AO))
        DI = tf.matmul(DO, tf.transpose(self.weights))

        DW = tf.matmul(tf.transpose(AI), DO)
        DB = tf.reduce_sum(DO, axis=0)

        if self.train_flag:
            return {"dout": DI, "cache": {}}, [(DW, self.weights), (DB, self.bias)]
        else:
            return {"dout": DI, "cache": {}}, []

    def dfa(self, AI, AO, E, DO, cache):
        return self.bp(AI, AO, DO, cache)

    def lel(self, AI, AO, DO, Y, cache):
        return self.bp(AI, AO, DO, cache)

    ###################################################################


"""## Model"""

np.set_printoptions(threshold=1000)


class Model:
    def __init__(self, layers: tuple):
        self.num_layers = len(layers)
        self.layers = layers

    def num_params(self):
        param_sum = 0
        for ii in range(self.num_layers):
            l = self.layers[ii]
            param_sum += l.num_params()
        return param_sum

    def get_weights(self):
        weights = {}
        for ii in range(self.num_layers):
            l = self.layers[ii]
            tup = l.get_weights()
            for key, value in tup:
                weights[key] = value

        return weights

    def predict(self, X):
        A = [None] * self.num_layers

        for ii in range(self.num_layers):
            l = self.layers[ii]
            if ii == 0:
                A[ii] = l.forward(X)
            else:
                A[ii] = l.forward(A[ii - 1]["aout"])

        return A[self.num_layers - 1]["aout"]

    ####################################################################

    def gvs(self, X, Y):
        A = [None] * self.num_layers
        D = [None] * self.num_layers
        grads_and_vars = []

        for ii in range(self.num_layers):
            l = self.layers[ii]
            if ii == 0:
                A[ii] = l.forward(X)
            else:
                A[ii] = l.forward(A[ii - 1]["aout"])

        E = tf.nn.softmax(A[self.num_layers - 1]["aout"]) - Y
        N = tf.shape(A[self.num_layers - 1]["aout"])[0]
        N = tf.cast(N, dtype=tf.float32)
        E = E / N

        for ii in range(self.num_layers - 1, -1, -1):
            l = self.layers[ii]

            if ii == self.num_layers - 1:
                D[ii], gvs = l.bp(A[ii - 1]["aout"], A[ii]["aout"], E, A[ii]["cache"])
                grads_and_vars.extend(gvs)
            elif ii == 0:
                D[ii], gvs = l.bp(X, A[ii]["aout"], D[ii + 1]["dout"], A[ii]["cache"])
                grads_and_vars.extend(gvs)
            else:
                D[ii], gvs = l.bp(
                    A[ii - 1]["aout"], A[ii]["aout"], D[ii + 1]["dout"], A[ii]["cache"]
                )
                grads_and_vars.extend(gvs)

        return grads_and_vars

    def dfa_gvs(self, X, Y):
        A = [None] * self.num_layers
        D = [None] * self.num_layers
        grads_and_vars = []

        for ii in range(self.num_layers):
            l = self.layers[ii]
            if ii == 0:
                A[ii] = l.forward(X)
            else:
                A[ii] = l.forward(A[ii - 1]["aout"])

        E = tf.nn.softmax(A[self.num_layers - 1]["aout"]) - Y
        N = tf.shape(A[self.num_layers - 1]["aout"])[0]
        N = tf.cast(N, dtype=tf.float32)
        E = E / N

        for ii in range(self.num_layers - 1, -1, -1):
            l = self.layers[ii]

            if ii == self.num_layers - 1:
                D[ii], gvs = l.dfa(
                    A[ii - 1]["aout"], A[ii]["aout"], E, E, A[ii]["cache"]
                )
                grads_and_vars.extend(gvs)
            elif ii == 0:
                D[ii], gvs = l.dfa(
                    X, A[ii]["aout"], E, D[ii + 1]["dout"], A[ii]["cache"]
                )
                grads_and_vars.extend(gvs)
            else:
                D[ii], gvs = l.dfa(
                    A[ii - 1]["aout"],
                    A[ii]["aout"],
                    E,
                    D[ii + 1]["dout"],
                    A[ii]["cache"],
                )
                grads_and_vars.extend(gvs)

        return grads_and_vars

    def lel_gvs(self, X, Y):
        A = [None] * self.num_layers
        D = [None] * self.num_layers
        grads_and_vars = []

        for ii in range(self.num_layers):
            l = self.layers[ii]
            if ii == 0:
                A[ii] = l.forward(X)
            else:
                A[ii] = l.forward(A[ii - 1]["aout"])

        E = tf.nn.softmax(A[self.num_layers - 1]["aout"]) - Y
        N = tf.shape(A[self.num_layers - 1]["aout"])[0]
        N = tf.cast(N, dtype=tf.float32)
        E = E / N

        for ii in range(self.num_layers - 1, -1, -1):
            l = self.layers[ii]

            if ii == self.num_layers - 1:
                D[ii], gvs = l.lel(
                    A[ii - 1]["aout"], A[ii]["aout"], E, E, Y, A[ii]["cache"]
                )
                grads_and_vars.extend(gvs)
            elif ii == 0:
                D[ii], gvs = l.lel(
                    X, A[ii]["aout"], E, D[ii + 1]["dout"], Y, A[ii]["cache"]
                )
                grads_and_vars.extend(gvs)
            else:
                D[ii], gvs = l.lel(
                    A[ii - 1]["aout"],
                    A[ii]["aout"],
                    E,
                    D[ii + 1]["dout"],
                    Y,
                    A[ii]["cache"],
                )
                grads_and_vars.extend(gvs)

        return grads_and_vars


################################################################

"""# Code Continues"""

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

train_examples = 50000
test_examples = 10000

assert np.shape(x_train) == (train_examples, 32, 32, 3)
x_train = x_train - np.mean(x_train, axis=0, keepdims=True)
x_train = x_train / np.std(x_train, axis=0, keepdims=True)
y_train = keras.utils.to_categorical(y_train, 10)

assert np.shape(x_test) == (test_examples, 32, 32, 3)
x_test = x_test - np.mean(x_test, axis=0, keepdims=True)
x_test = x_test / np.std(x_test, axis=0, keepdims=True)
y_test = keras.utils.to_categorical(y_test, 10)

if args.act == "tanh":
    act = Tanh()
elif args.act == "relu":
    act = Relu()
else:
    assert False

# print(args)

# tf.compat.v1.disable_eager_execution()# import random
tf.set_random_seed(0)
# tf.random.set_seed(0)

tf.reset_default_graph()
# tf.keras.backend.clear_session()

tf.compat.v1.disable_eager_execution()
batch_size = tf.placeholder(tf.int32, shape=())
# batch_size = tf.constant(shape=(), dtype=tf.int32) # error requires value
# batch_size = tf.keras.Input(shape=(), dtype=tf.int32)

dropout_rate = tf.placeholder(tf.float32, shape=())
# dropout_rate = tf.constant(shape=(), dtype=tf.float32)
# dropout_rate = tf.keras.Input(shape=(), dtype=tf.float32)
lr = tf.placeholder(tf.float32, shape=())
# lr = tf.constant(shape=(), dtype=tf.float32)
# lr = tf.keras.Input(shape=(), dtype=tf.float32)

X = tf.placeholder(tf.float32, [None, 32, 32, 3])
# X = tf.constant(shape= (None, 32, 32, 3), dtype=tf.float32)
# X = tf.keras.Input(shape=(None,32,32,3), dtype=tf.float32)

Y = tf.placeholder(tf.float32, [None, 10])
# Y = tf.constant(shape= (None, 32,32,3), dtype=tf.float32)
# Y = tf.keras.Input(shape=(None,32,32,3), dtype=tf.float32)

"""## Original layers"""

l0 = Convolution(
    input_shape=[batch_size, 32, 32, 3],
    filter_sizes=[5, 5, 3, 96],
    init=args.init,
    activation=act,
    bias=args.bias,
    name="conv1",
)
l1 = MaxPool(
    size=[batch_size, 32, 32, 96],
    ksize=[1, 3, 3, 1],
    strides=[1, 2, 2, 1],
    padding="SAME",
)
l2 = FeedbackConv(
    size=[batch_size, 16, 16, 96],
    num_classes=10,
    sparse=args.sparse,
    rank=args.rank,
    name="conv1_fb",
)

l3 = Convolution(
    input_shape=[batch_size, 16, 16, 96],
    filter_sizes=[5, 5, 96, 128],
    init=args.init,
    activation=act,
    bias=args.bias,
    name="conv2",
)
l4 = MaxPool(
    size=[batch_size, 16, 16, 128],
    ksize=[1, 3, 3, 1],
    strides=[1, 2, 2, 1],
    padding="SAME",
)
l5 = FeedbackConv(
    size=[batch_size, 8, 8, 128],
    num_classes=10,
    sparse=args.sparse,
    rank=args.rank,
    name="conv2_fb",
)

l6 = Convolution(
    input_shape=[batch_size, 8, 8, 128],
    filter_sizes=[5, 5, 128, 256],
    init=args.init,
    activation=act,
    bias=args.bias,
    name="conv3",
)
l7 = MaxPool(
    size=[batch_size, 8, 8, 256],
    ksize=[1, 3, 3, 1],
    strides=[1, 2, 2, 1],
    padding="SAME",
)
l8 = FeedbackConv(
    size=[batch_size, 4, 4, 256],
    num_classes=10,
    sparse=args.sparse,
    rank=args.rank,
    name="conv3_fb",
)

l9 = ConvToFullyConnected(input_shape=[4, 4, 256])

l10 = FullyConnected(
    input_shape=4 * 4 * 256,
    size=2048,
    init=args.init,
    activation=act,
    bias=args.bias,
    name="fc1",
)
l11 = Dropout(rate=dropout_rate)
l12 = FeedbackFC(
    size=[4 * 4 * 256, 2048],
    num_classes=10,
    sparse=args.sparse,
    rank=args.rank,
    name="fc1_fb",
)

l13 = FullyConnected(
    input_shape=2048,
    size=2048,
    init=args.init,
    activation=act,
    bias=args.bias,
    name="fc2",
)
l14 = Dropout(rate=dropout_rate)
l15 = FeedbackFC(
    size=[2048, 2048], num_classes=10, sparse=args.sparse, rank=args.rank, name="fc2_fb"
)

l16 = FullyConnected(
    input_shape=2048, size=10, init=args.init, bias=args.bias, name="fc3"
)

model = Model(
    layers=[l0, l1, l2, l3, l4, l5, l6, l7, l8, l9, l10, l11, l12, l13, l14, l15, l16]
)
# model = Model(layers=[l0, l1, l2, l3, l4, l5, l6, l7, l8, l9, l10, l11, l12, l13, l14, l15, l16])
predict = model.predict(X=X)
weights = model.get_weights()

if args.dfa:
    grads_and_vars = model.dfa_gvs(X=X, Y=Y)
else:
    grads_and_vars = model.gvs(X=X, Y=Y)

train = tf.train.AdamOptimizer(learning_rate=lr, epsilon=args.eps).apply_gradients(
    grads_and_vars=grads_and_vars
)

correct = tf.equal(tf.argmax(predict, 1), tf.argmax(Y, 1))
total_correct = tf.reduce_sum(tf.cast(correct, tf.float32))

sess = tf.InteractiveSession()
tf.global_variables_initializer().run()
tf.local_variables_initializer().run()

filename = args.name + ".results"
f = open(filename, "w")
f.write(filename + "\n")
f.write("total params: " + str(model.num_params()) + "\n")
f.close()

train_accs = []
test_accs = []

for ii in range(args.epochs):
    _total_correct = 0
    for jj in range(0, train_examples, args.batch_size):
        s = jj
        e = min(jj + args.batch_size, train_examples)
        b = e - s

        xs = x_train[s:e]
        ys = y_train[s:e]

        _correct, _ = sess.run(
            [total_correct, train],
            feed_dict={
                batch_size: b,
                dropout_rate: args.dropout,
                lr: args.lr,
                X: xs,
                Y: ys,
            },
        )
        _total_correct += _correct

    train_acc = (
        1.0 * _total_correct / (train_examples - (train_examples % args.batch_size))
    )
    train_accs.append(train_acc)

    _total_correct = 0
    for jj in range(0, test_examples, args.batch_size):
        s = jj
        e = min(jj + args.batch_size, test_examples)
        b = e - s

        xs = x_test[s:e]
        ys = y_test[s:e]

        _correct = sess.run(
            total_correct,
            feed_dict={batch_size: b, dropout_rate: 0.0, lr: 0.0, X: xs, Y: ys},
        )
        _total_correct += _correct

    test_acc = (
        1.0 * _total_correct / (test_examples - (test_examples % args.batch_size))
    )
    # print("epoch : ",ii, " --> Training acc : ",train_acc, " | Test acc : ",test_acc)
    test_accs.append(test_acc)
    p = "epoch : %d | train acc: %f | test acc: %f" % (ii, train_acc, test_acc)
    print(p)
    f = open(filename, "a")
    f.write(p + "\n")
    f.close()

print(train_accs)

print(args.epochs)
