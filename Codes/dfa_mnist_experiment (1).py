# -*- coding: utf-8 -*-
"""dfa_MNIST_experiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11SPg5hE81n4Svj3afgn9W3GBm5j7EG3m

# DIRECT FEEDBACK ALIGNMENT ON MNIST
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
from keras.datasets import mnist
# from keras.datasets import cifar10
# from keras.utils.np_utils import to_categorical
from tensorflow.python.keras.utils.np_utils import to_categorical
np.random.seed(1234) # fix first random number
# %matplotlib inline

"""## CREATING DATASET"""

(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train /= 255 # normalize data
X_test /= 255 # normalize data

print('Input dimensions')
print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)

X_train = X_train.reshape(60000, 28*28)
X_test = X_test.reshape(10000, 28*28)

print('After reshaping:', X_train.shape, X_test.shape)

print('Sample of label:', y_train[0])

nb_classes = 10
y_train = to_categorical(y_train, nb_classes)
y_test = to_categorical(y_test, nb_classes)

print('After conversion to categorical:', y_train[0])

"""## DEFINITION OF FORWARD AND BACKWARD PASS"""

# import of sigmoid and crossentropy
from scipy.special import expit
from sklearn.metrics import log_loss

def forward_pass3(W1,W2,W3,W4,b1,b2,b3,b4,x):
    # if the input is a batch, I have to tile as many
    # b1 and b2 as the batch size
    a1 = np.matmul(W1, x)+np.tile(b1, x.shape[1])
    h1 = np.tanh(a1)

    a2 = np.matmul(W2, h1)+np.tile(b2, x.shape[1])
    h2 = np.tanh(a2)

    a3 = np.matmul(W3, h2)+np.tile(b3, x.shape[1])
    h3 = np.tanh(a3)

    a4 = np.matmul(W4, h3)+np.tile(b4, x.shape[1])
    y_hat = expit(a4)
    return a1, h1, a2, h2, a3, h3, a4, y_hat

def backprop_backward_pass3(e, h1, h2, h3, W2, W3, W4, a1, a2, a3, x):
    dW4 =  -np.matmul(e,np.transpose(h3))
    db4 = -np.sum(e,axis=1)

    da3 = np.matmul(np.transpose(W4), e)*(1-np.tanh(a3)**2)
    dW3 = -np.matmul(da3,np.transpose(h2))
    db3 = -np.sum(da3, axis=1)

    da2 = np.matmul(np.transpose(W3), da3)*(1-np.tanh(a2)**2)
    dW2 = -np.matmul(da2, np.transpose(h1))
    db2 = -np.sum(da2,axis=1)

    da1 = np.matmul(np.transpose(W2), da2)*(1-np.tanh(a1)**2)
    dW1 = -np.matmul(da1, np.transpose(x))
    db1 = -np.sum(da1, axis=1)

    return dW1, dW2, dW3, dW4, db1[:,np.newaxis], db2[:,np.newaxis], db3[:,np.newaxis], db4[:,np.newaxis]

"""## DEFINITION OF FUNCTION TO COMPUTE ANGLE BETWEEN UPDATES"""

def train2(x, y, n_epochs=10, lr=1e-3, batch_size=200, tol=1e-1):
    x = np.transpose(x)
    y = np.transpose(y)

    W1, W2, W3, W4 = np.random.randn(800, 784), np.random.randn(800,800), np.random.randn(800,800), np.random.randn(10, 800)
    b1, b2, b3, b4 = np.random.randn(800, 1), np.random.randn(800,1), np.random.randn(800,1), np.random.randn(10, 1)

    dataset_size = x.shape[1]
    n_batches = dataset_size//batch_size
    te_bp = []
    for i in range(n_epochs):
        perm = np.random.permutation(x.shape[1])
        x = x[:, perm]
        y = y[:, perm]
        loss = 0.
        train_error = 0.
        for j in range(n_batches):
            samples = x[:, j*batch_size:(j+1)*batch_size]
            targets = y[:, j*batch_size:(j+1)*batch_size]
            a1, h1, a2, h2, a3, h3, a4, y_hat = forward_pass3(W1, W2, W3, W4, b1, b2, b3, b4, samples)
            error = y_hat - targets
            preds = np.argmax(y_hat, axis=0)
            truth = np.argmax(targets, axis=0)
            train_error += np.sum(preds!=truth)
            loss_on_batch = log_loss(targets, y_hat)

            dW1, dW2, dW3, dW4, db1, db2, db3, db4 = backprop_backward_pass3(error, h1, h2, h3, W2, W3, W4, a1, a2, a3,samples)
            W1 += lr*dW1
            W2 += lr*dW2
            W3 += lr*dW3
            W4 += lr*dW4
            b1 += lr*db1
            b2 += lr*db2
            b3 += lr*db3
            b4 += lr*db4
            loss += loss_on_batch
        training_error = 1.*train_error/x.shape[1]
        print('Loss at epoch', i+1, ':', loss/x.shape[1])
        print('Training error:', training_error)
        prev_training_error = 0 if i==0 else te_bp[-1]
        if np.abs(training_error-prev_training_error) <= tol:
            te_bp.append(training_error)
            break
        te_bp.append(training_error)
    return W1, W2, W3, W4, b1, b2, b3, b4, te_bp

# print(X_train)
W1, W2, W3, W4, b1, b2, b3, b4, te_bp = train2(X_train, y_train, n_epochs=100, lr=1e-4, batch_size=200, tol=1e-4)

def dfa_backward_pass3(e, h1, h2, h3, B1, B2, B3, a1, a2, a3, x):
    dW4 = -np.matmul(e,np.transpose(h3))
    db4 = -np.sum(e,axis=1)

    da3 = np.matmul(B3, e)*(1-np.tanh(a3)**2)
    dW3 = -np.matmul(da3,np.transpose(h2))
    db3 = -np.sum(da3, axis=1)

    da2 = np.matmul(B2, e)*(1-np.tanh(a2)**2)
    dW2 = -np.matmul(da2, np.transpose(h1))
    db2 = -np.sum(da2,axis=1)

    da1 = np.matmul(B1, e)*(1-np.tanh(a1)**2)
    dW1 = -np.matmul(da1, np.transpose(x))
    db1 = -np.sum(da1, axis=1)


    return dW1, dW2, dW3, dW4, db1[:,np.newaxis], db2[:,np.newaxis], db3[:,np.newaxis], db4[:,np.newaxis]

def dfa_train2(x, y, n_epochs=10, lr=1e-3, batch_size=200, tol=1e-3):
    x = np.transpose(x)
    y = np.transpose(y)

    W1, W2, W3, W4 = np.random.randn(800, 784), np.random.randn(800,800), np.random.randn(800,800), np.random.randn(10, 800)
    b1, b2, b3, b4 = np.random.randn(800, 1), np.random.randn(800,1), np.random.randn(800,1), np.random.randn(10, 1)

    B1 = np.random.randn(800,10)
    B2 = np.random.randn(800,10)
    B3 = np.random.randn(800,10)
    dataset_size = x.shape[1]
    n_batches = dataset_size//batch_size
    te_dfa = []
    angles = []
    for i in range(n_epochs):
        perm = np.random.permutation(x.shape[1])
        x = x[:, perm]
        y = y[:, perm]
        loss = 0.
        train_error = 0.
        for j in range(n_batches):
            samples = x[:, j*batch_size:(j+1)*batch_size]
            targets = y[:, j*batch_size:(j+1)*batch_size]
            a1, h1, a2, h2, a3, h3, a4, y_hat = forward_pass3(W1, W2, W3, W4, b1, b2, b3, b4, samples)
            error = y_hat - targets
            preds = np.argmax(y_hat, axis=0)
            truth = np.argmax(targets, axis=0)
            train_error += np.sum(preds!=truth)
            loss_on_batch = log_loss(targets, y_hat)

            dW1, dW2, dW3, dW4, db1, db2, db3, db4 = dfa_backward_pass3(error, h1, h2, h3, B1, B2, B3, a1, a2, a3, samples)
            W1 += lr*dW1
            W2 += lr*dW2
            W3 += lr*dW3
            W4 += lr*dW4
            b1 += lr*db1
            b2 += lr*db2
            b3 += lr*db3
            b4 += lr*db4
            loss += loss_on_batch
        training_error = 1.*train_error/x.shape[1]
        print('Loss at epoch', i+1, ':', loss/x.shape[1])
        print('Training error:', training_error)
        prev_training_error = 0 if i==0 else te_dfa[-1]
        if np.abs(training_error-prev_training_error) <= tol:
            te_dfa.append(training_error)
            break
        te_dfa.append(training_error)
    return W1, W2, W3, W4, b1, b2, b3, b4, te_dfa, angles

W1dfa, W2dfa, W3dfa, W4dfa, b1dfa, b2dfa, b3dfa, b4dfa, te_dfa, angles = dfa_train2(X_train, y_train, n_epochs=100, lr=1e-4, batch_size=200, tol=1e-4)

def test(W1, W2, W3, W4, b1, b2, b3, b4, test_samples, test_targets):
    test_samples = np.transpose(test_samples)
    test_targets = np.transpose(test_targets)
    # outs = forward_pass1(W1, W2, b1, b2, test_samples)[-1]
    outs = forward_pass3(W1, W2, W3, W4, b1, b2, b3, b4, test_samples)[-1]
    preds = np.argmax(outs, axis=0)
    truth = np.argmax(test_targets, axis=0)
    test_error = 1.*np.sum(preds!=truth)/preds.shape[0]
    return test_error

print('BP test error:', test(W1, W2 ,W3, W4, b1, b2, b3, b4, X_test, y_test)*100, '%')
print('DFA test error:', test(W1dfa, W2dfa, W3dfa, W4dfa, b1dfa, b2dfa, b3dfa, b4dfa, X_test, y_test)*100, '%')

plt.plot(range(len(te_bp)), te_bp, label='BP training error')
plt.plot(range(len(te_dfa)), te_dfa, label='DFA training error')
plt.title('Learning rate 1e-4')
plt.xlabel('Epochs')
plt.ylabel('Training error %')
plt.legend(loc='best')
plt.savefig("output.jpg")
plt.show()

l, beta = zip(*angles)

plt.plot(range(len(beta)), beta, label='angle')
plt.legend(loc='best')
plt.xlabel('Epoch*3')
plt.ylabel('Angle')
plt.show()