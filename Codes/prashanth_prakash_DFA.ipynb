{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uMQB6pcU4Bu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import pickle\n",
        "import numpy.random as rng"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SjRO_4hYyxq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "aE0v4ZgGu-lN"
      },
      "outputs": [],
      "source": [
        "def load_data(file_no):\n",
        "    path='/content/drive/MyDrive/datasets/IP_dataset/data_batch_'+str(file_no)\n",
        "    #path='/content/drive/MyDrive/IP_dataset/data_batch_'+str(file_no)\n",
        "    fo=open(path,'rb')\n",
        "    dict=pickle.load(fo,encoding='bytes')\n",
        "    X=dict[b'data']\n",
        "    Y=dict[b'labels']\n",
        "    fo.close\n",
        "    X=X.reshape((len(X),3,32,32)).transpose(0,2,3,1).astype(\"uint8\")\n",
        "    Y=np.array(Y)\n",
        "    Y_hot=np.eye(no_of_classes)[Y]\n",
        "    return X,Y_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "dEwNdzPb030i"
      },
      "outputs": [],
      "source": [
        "fo_tst=open('/content/drive/MyDrive/datasets/IP_dataset/test_batch','rb')\n",
        "#fo_tst=open('/content/drive/MyDrive/IP_dataset/test_batch','rb')\n",
        "dict=pickle.load(fo_tst,encoding='bytes')\n",
        "X_tst=dict[b'data']\n",
        "Y_tst=dict[b'labels']\n",
        "fo_tst.close\n",
        "X_tst=X_tst.reshape((len(X_tst),3,32,32)).transpose(0,2,3,1).astype(\"uint8\")\n",
        "Y_tst=np.array(Y_tst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pyHqc9Fg0-5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7f3186-42b5-4891-8a8b-f163fd050dc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "X_tst.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yzn4c9mj1Fqs"
      },
      "outputs": [],
      "source": [
        "def forward_conv(height,width,inshape,outshape,input):\n",
        "    weights=tf.Variable(rng.randn(height,width,inshape,outshape), dtype = tf.float32,name='conv_weights') #constant\n",
        "    return(tf.nn.conv2d(input,weights,strides=[1,1,1,1],padding=\"SAME\"))\n",
        "\n",
        "def forward_max_pooling_layer(inp,window_size):\n",
        "    return(tf.nn.max_pool(value=inp,ksize=[1,window_size,window_size,1],strides=[1,1,1,1],padding=\"SAME\"))\n",
        "\n",
        "def forward_avg_pooling_layer(inp,window_size):\n",
        "    return(tf.nn.avg_pool(value=inp,ksize=[1,window_size,window_size,1],strides=[1,1,1,1],padding=\"SAME\"))\n",
        "\n",
        "def flatten_forward(layer):\n",
        "    inp_list=layer.get_shape().as_list()\n",
        "    new_size = inp_list[-1] * inp_list[-2] * inp_list[-3]\n",
        "    return tf.reshape(layer,[-1,new_size]),new_size\n",
        "\n",
        "def fc_forward(layer,new_size,no_of_classes):\n",
        "    weights=tf.Variable(rng.randn(new_size,no_of_classes),dtype=tf.float32,name='fc_forward_weights') #constant\n",
        "    return tf.matmul(layer,weights)\n",
        "\n",
        "def fc_fc(rows,columns,layers):\n",
        "    weights=tf.Variable(rng.randn(rows,columns),dtype=tf.float32,name='fc_fc_weights')\n",
        "    return tf.matmul(layers,weights)\n",
        "\n",
        "def activation(layer):\n",
        "    return tf.nn.relu(layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ygBP4Yo82uUO"
      },
      "outputs": [],
      "source": [
        "@tf.RegisterGradient(\"CustomConv\")\n",
        "def _conv2d(op,grad):\n",
        "    print(\"in override backprop\")\n",
        "    input = op.inputs[0]\n",
        "    filter = op.inputs[1]\n",
        "    in_shape = tf.shape(input)\n",
        "    f_shape = tf.shape(filter)\n",
        "    g_input = tf.nn.conv2d_backprop_input(input_sizes = in_shape, filter = filter, out_backprop = grad, strides = [1,1,1,1], padding = \"SAME\")\n",
        "    g_filter = tf.nn.conv2d_backprop_filter(input, filter_sizes = f_shape, out_backprop = grad, strides = [1,1,1,1], padding = \"SAME\")\n",
        "    return g_input, g_filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GeujsE2_1QLm"
      },
      "outputs": [],
      "source": [
        "num_epochs=150\n",
        "# batch=32\n",
        "batch=1000\n",
        "iterations=1000\n",
        "# iterations=10\n",
        "no_of_classes=10\n",
        "\n",
        "Y_hot_tst=np.eye(no_of_classes)[Y_tst]\n",
        "\n",
        "\n",
        "images=tf.placeholder(tf.float32,shape=(None,32,32,3),name='images')\n",
        "true_labels=tf.placeholder(tf.float32,shape=(None,10),name='true_labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "uscd1W-Q1TUz"
      },
      "outputs": [],
      "source": [
        "filter_random1 = tf.Variable(rng.randn(5,5,3,32), dtype = tf.float32,name='random_filter1')\n",
        "\n",
        "g=tf.get_default_graph()\n",
        "with g.gradient_override_map({\"Conv2D\": \"CustomConv\"}):\n",
        "    net_conv=tf.nn.conv2d(images,filter_random1,strides=[1,1,1,1],padding=\"SAME\")\n",
        "\n",
        "net_pool= forward_max_pooling_layer(net_conv,3)\n",
        "net_act=activation(net_pool)\n",
        "\n",
        "#LAYER 2\n",
        "filter_random2 = tf.Variable(rng.randn(5,5,32,64), dtype = tf.float32,name='random_filter2')\n",
        "with g.gradient_override_map({\"Conv2D\":\"CustomConv\"}):\n",
        "    net_conv2=tf.nn.conv2d(net_act,filter_random2,strides=[1,1,1,1],padding=\"SAME\")\n",
        "net_pool2= forward_avg_pooling_layer(net_conv2,3)\n",
        "net_act2=activation(net_pool2)\n",
        "\n",
        "#LAYER 3\n",
        "filter_random3=tf.Variable(rng.randn(5,5,64,64),dtype=tf.float32,name='random_filter3')\n",
        "with g.gradient_override_map({\"Conv2D\":\"CustomConv\"}):\n",
        "    net_conv3=tf.nn.conv2d(net_act2,filter_random3,strides=[1,1,1,1],padding=\"SAME\")\n",
        "net_pool3=forward_avg_pooling_layer(net_conv3,3)\n",
        "\n",
        "net_act3=activation(net_pool3)\n",
        "\n",
        "net_flatten,new_size=flatten_forward(net_act3)\n",
        "\n",
        "net_fc=fc_forward(net_flatten,new_size,128)\n",
        "net_act4=activation(net_fc)\n",
        "output=fc_fc(128,no_of_classes,net_act4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "P1XwTWmm28p9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4138ac-e9cd-4ad5-feaf-82209ed78dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cross_entropy=tf.nn.softmax_cross_entropy_with_logits(logits=output,labels=true_labels)\n",
        "cost = tf.reduce_mean(cross_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "-qiY1dbN3AOe"
      },
      "outputs": [],
      "source": [
        "# # Back Prop\n",
        "\n",
        "# filter_conv_bp=tf.Variable(rng.randn(5,5,3,16), dtype = tf.float32)\n",
        "\n",
        "#LAYER 1\n",
        "net_conv_bp= forward_conv(5,5,3,32,images) # height,width,inshape,outshape\n",
        "net_pool_bp = forward_max_pooling_layer(net_conv_bp,3) #output,windowsize\n",
        "net_act_bp=activation(net_pool_bp)\n",
        "\n",
        "#LAYER 2\n",
        "net_conv_bp2 = forward_conv(5,5,32,64,net_act_bp) # height,width,inshape,outshape\n",
        "net_pool_bp2 = forward_avg_pooling_layer(net_conv_bp2,3) #output,windowsize\n",
        "net_act_bp2=activation(net_pool_bp2)\n",
        "\n",
        "#LAYER 3\n",
        "net_conv_bp3=forward_conv(5,5,64,64,net_act_bp2)\n",
        "net_pool_bp3=forward_avg_pooling_layer(net_conv_bp3,3)\n",
        "net_act_bp3=activation(net_pool_bp3)\n",
        "\n",
        "net_flatten_bp,new_size_bp=flatten_forward(net_act_bp3)\n",
        "net_fc_bp1=fc_forward(net_flatten_bp,new_size_bp,128)\n",
        "net_act_bp4=activation(net_fc_bp1)\n",
        "output_bp=fc_fc(128,no_of_classes,net_act_bp4)\n",
        "\n",
        "\n",
        "#cross_entropy_bp=tf.nn.softmax_cross_entropy_with_logits_v2(logits=\n",
        "#                                  output_bp,labels=true_labels)\n",
        "cross_entropy_bp=tf.nn.softmax_cross_entropy_with_logits(logits=\n",
        "                                  output_bp,labels=true_labels)\n",
        "cost_bp=tf.reduce_mean(cross_entropy_bp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ayGjMlcZ3Le9"
      },
      "outputs": [],
      "source": [
        "# In[10]:\n",
        "\n",
        "accuracy_fa=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output,1),tf.argmax(true_labels,1)),tf.float32))\n",
        "accuracy_bp=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output_bp,1),tf.argmax(true_labels,1)),tf.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4OnEzHxN3VA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25319c3-af14-4f75-fc3e-c33c152f7948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in override backprop\n",
            "in override backprop\n",
            "in override backprop\n"
          ]
        }
      ],
      "source": [
        "#BP gradients\n",
        "bp_grad = tf.gradients(cross_entropy_bp, images)\n",
        "\n",
        "override_grad = tf.gradients(cross_entropy, images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "0rZyPEoo3YSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e89e707-4ca7-407d-90e7-33cb4012efbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in override backprop\n",
            "in override backprop\n",
            "in override backprop\n"
          ]
        }
      ],
      "source": [
        "# In[20]:\n",
        "\n",
        "train_op_bp = tf.train.AdamOptimizer(1e-4).minimize(cost_bp)#changed learning rate from 1e-6\n",
        "train_op_fa=tf.train.AdamOptimizer(1e-4).minimize(cost)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKpvKIzr3bow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50008ac0-069c-47d7-d66a-ced52b099b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PARAMETERS:\n",
            "\n",
            "No of epochs= 150\n",
            "\n",
            "Batch size= 1000\n",
            "\n",
            "Iterations per epoch= 1000\n",
            "\n",
            "\t\t\tEPOCH NO: 1\n",
            "Picking from data batch: 4\n"
          ]
        }
      ],
      "source": [
        "# In[ ]:\n",
        "\n",
        "store_err_bp=[]\n",
        "store_err_fa=[]\n",
        "acc_fa=[]\n",
        "acc_bp=[]\n",
        "testing_fa=[]\n",
        "testing_bp=[]\n",
        "\n",
        "print(\"PARAMETERS:\")\n",
        "print(\"\\nNo of epochs=\",num_epochs)\n",
        "print(\"\\nBatch size=\",batch)\n",
        "print(\"\\nIterations per epoch=\",iterations)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"\\n\\t\\t\\tEPOCH NO:\",epoch+1)\n",
        "        no=np.random.randint(1,5)\n",
        "        X,Y_hot=load_data(no)\n",
        "        print(\"Picking from data batch:\",no)\n",
        "        batch_no=np.random.randint(0,X.shape[0],size=batch)\n",
        "\n",
        "        for count in range(iterations):\n",
        "            inp_features=X[batch_no,:,:,:]\n",
        "            inp_features=inp_features.astype(np.float32)\n",
        "\n",
        "            inp_labels=Y_hot[batch_no,:]\n",
        "            inp_labels=inp_labels.astype(np.float32)\n",
        "\n",
        "#             autobp_input=sess.run(bp_grad,feed_dict={images:inp_features,true_labels:inp_labels})\n",
        "#             override_input=sess.run(override_grad,feed_dict={images:inp_features,true_labels:inp_labels})\n",
        "\n",
        "            sess.run(train_op_bp,feed_dict={images:inp_features,true_labels:inp_labels})\n",
        "            sess.run(train_op_fa,feed_dict={images:inp_features,true_labels:inp_labels})\n",
        "\n",
        "            entropy_bp=sess.run(cross_entropy_bp,feed_dict={images:inp_features,true_labels:inp_labels})\n",
        "            store_err_bp.append(np.mean(entropy_bp))\n",
        "            entropy_fa=sess.run(cross_entropy,feed_dict={images:inp_features,true_labels:inp_labels})\n",
        "            store_err_fa.append(np.mean(entropy_fa))\n",
        "\n",
        "            acc_fa.append(sess.run(accuracy_fa,feed_dict={images:inp_features,true_labels:inp_labels}))\n",
        "            acc_bp.append(sess.run(accuracy_bp,feed_dict={images:inp_features,true_labels:inp_labels}))\n",
        "\n",
        "\n",
        "            if (count+1)%200==0:\n",
        "                print(\"Iteration:\",count+1)\n",
        "                print(\"BackPropagation:\",sess.run(cost_bp,feed_dict={images:inp_features,true_labels:inp_labels}),\n",
        "                      \"\\t Feedback:\",sess.run(cost,feed_dict={images:inp_features,true_labels:inp_labels}))\n",
        "\n",
        "\n",
        "        #Resource Exhausted with loading the entire testing data which has 10000 images.\n",
        "        #Instead testing on 1000 random images selected from the testing data. results may not be as consistent\n",
        "\n",
        "        pick_rnd=np.random.randint(0,X_tst.shape[0],1000)\n",
        "        tst_fa=sess.run(accuracy_fa,feed_dict={images:X_tst[pick_rnd,:,:,:].astype(np.float32),\n",
        "                                               true_labels:Y_hot_tst[pick_rnd,:]})\n",
        "        tst_bp=sess.run(accuracy_bp,feed_dict={images:X_tst[pick_rnd,:,:,:].astype(np.float32),\n",
        "                                               true_labels:Y_hot_tst[pick_rnd,:]})\n",
        "\n",
        "        testing_fa.append(tst_fa)\n",
        "        testing_bp.append(tst_bp)\n",
        "\n",
        "        print(\"\\nAt the end of EPOCH:\",epoch+1)\n",
        "        print(\"Testing Accuracy:\\nBack Propagation\",tst_bp,\"\\tRandom Feedback:\",tst_fa)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol-P45ZjvQdB"
      },
      "outputs": [],
      "source": [
        "with open('Analysis-CIFAR10_1e-4.pkl','wb') as f:\n",
        "    pickle.dump([store_err_bp,store_err_fa,acc_fa,acc_bp,testing_fa,testing_bp],f,protocol=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}